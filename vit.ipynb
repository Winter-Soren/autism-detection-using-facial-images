{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib --quiet\n",
    "%pip install numpy --quiet\n",
    "%pip install pandas --quiet\n",
    "%pip install torch --quiet\n",
    "%pip install linformer --quiet\n",
    "%pip install Pillow --quiet\n",
    "%pip install torchvision --quiet\n",
    "%pip install sklearn --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm --quiet\n",
    "%pip install vit-pytorch --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from vit_pytorch.efficient import ViT\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "batch_size = 64 \n",
    "epochs = 20\n",
    "lr = 3e-5\n",
    "gamma = 0.7\n",
    "seed = 142\n",
    "IMG_SIZE = 128\n",
    "patch_size = 16\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Transforms (with Augmentation) and Pytorch Preprocessing:\n",
    "train_ds = torchvision.datasets.ImageFolder(\"dataset/dataset/train\", transform=ToTensor())\n",
    "valid_ds = torchvision.datasets.ImageFolder(\"dataset/dataset/val\", transform=ToTensor())\n",
    "test_ds = torchvision.datasets.ImageFolder(\"dataset/dataset/test\", transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders:\n",
    "train_loader = data.DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "valid_loader = data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training device:\n",
    "device = 'cpu'\n",
    "\n",
    "# Linear Transformer:\n",
    "efficient_transformer = Linformer(dim=128, seq_len=64+1, depth=12, heads=8, k=64)\n",
    "\n",
    "# Vision Transformer Model: \n",
    "model = ViT(dim=128, image_size=128, patch_size=patch_size, num_classes=num_classes, transformer=efficient_transformer, channels=3).to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Learning Rate Scheduler for Optimizer:\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m epoch_accuracy \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     10\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfor\u001b[39;00m data, label \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     13\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m     label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\soham\\Desktop\\CODE\\Research\\AutismDetectionUsingFacialImages\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    model.train()\n",
    "    \n",
    "    for data, label in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        model.eval()\n",
    "\n",
    "        for data, label in valid_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(valid_loader)\n",
    "            epoch_val_loss += val_loss.item() / len(valid_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1} - Loss: {epoch_loss:.4f} - Acc: {epoch_accuracy:.4f} - Val Loss: {epoch_val_loss:.4f} - Val Acc: {epoch_val_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model:\n",
    "PATH = \"epochs\"+\"_\"+str(epochs)+\"_\"+\"img\"+\"_\"+str(IMG_SIZE)+\"_\"+\"patch\"+\"_\"+str(patch_size)+\"_\"+\"lr\"+\"_\"+str(lr)+\".pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model:\n",
    "PATH = \"epochs\"+\"_\"+str(epochs)+\"_\"+\"img\"+\"_\"+str(IMG_SIZE)+\"_\"+\"patch\"+\"_\"+str(patch_size)+\"_\"+\"lr\"+\"_\"+str(lr)+\".pt\"\n",
    "efficient_transformer = Linformer(dim=128, seq_len=49+1, depth=12, heads=8, k=64)\n",
    "model = ViT(image_size=224, patch_size=32, num_classes=2, dim=128 ,transformer=efficient_transformer, channels=3)\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on Valid/Test Data\n",
    "def overall_accuracy(model, test_loader, criterion):\n",
    "    \n",
    "    '''\n",
    "    Model testing \n",
    "    \n",
    "    Args:\n",
    "        model: model used during training and validation\n",
    "        test_loader: data loader object containing testing data\n",
    "        criterion: loss function used\n",
    "    \n",
    "    Returns:\n",
    "        test_loss: calculated loss during testing\n",
    "        accuracy: calculated accuracy during testing\n",
    "        y_proba: predicted class probabilities\n",
    "        y_truth: ground truth of testing data\n",
    "    '''\n",
    "    \n",
    "    y_proba = []\n",
    "    y_truth = []\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data in tqdm(test_loader):\n",
    "        X, y = data[0].to('cpu'), data[1].to('cpu')\n",
    "        output = model(X)\n",
    "        test_loss += criterion(output, y.long()).item()\n",
    "        for index, i in enumerate(output):\n",
    "            y_proba.append(i[1])\n",
    "            y_truth.append(y[index])\n",
    "            if torch.argmax(i) == y[index]:\n",
    "                correct+=1\n",
    "            total+=1\n",
    "                \n",
    "    accuracy = correct/total\n",
    "    \n",
    "    y_proba_out = np.array([float(y_proba[i]) for i in range(len(y_proba))])\n",
    "    y_truth_out = np.array([float(y_truth[i]) for i in range(len(y_truth))])\n",
    "    \n",
    "    return test_loss, accuracy, y_proba_out, y_truth_out\n",
    "\n",
    "\n",
    "loss, acc, y_proba, y_truth = overall_accuracy(model, test_loader, criterion = nn.CrossEntropyLoss())\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "\n",
    "print(pd.value_counts(y_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve:\n",
    "\n",
    "def plot_ROCAUC_curve(y_truth, y_proba, fig_size):\n",
    "    \n",
    "    '''\n",
    "    Plots the Receiver Operating Characteristic Curve (ROC) and displays Area Under the Curve (AUC) score.\n",
    "    \n",
    "    Args:\n",
    "        y_truth: ground truth for testing data output\n",
    "        y_proba: class probabilties predicted from model\n",
    "        fig_size: size of the output pyplot figure\n",
    "    \n",
    "    Returns: void\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, threshold = roc_curve(y_truth, y_proba)\n",
    "    auc_score = roc_auc_score(y_truth, y_proba)\n",
    "    txt_box = \"AUC Score: \" + str(round(auc_score, 4))\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1],'--')\n",
    "    plt.annotate(txt_box, xy=(0.65, 0.05), xycoords='axes fraction')\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "#     plt.savefig('ROC.png')\n",
    "plot_ROCAUC_curve(y_truth, y_proba, (8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "net = model\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "        output = net(inputs) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = ('cats', 'dogs')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix), index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "# plt.savefig('cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on Single Images (cats-dogs):\n",
    "test_image = \"new_cat_image.jpg\"\n",
    "test_image_null = \"new_dog_image.png\"\n",
    "image = Image.open(test_image)\n",
    "image_null = Image.open(test_image_null)\n",
    "\n",
    "# Define tensor transform and apply it:\n",
    "data_transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "image_t = data_transform(image).unsqueeze(0)\n",
    "image_null_t = data_transform(image_null).unsqueeze(0)\n",
    "\n",
    "# Labels:\n",
    "for inputs, labels in test_loader:\n",
    "        labels = labels.data.cpu().numpy()\n",
    "\n",
    "# Prediction:\n",
    "out_cat = model(image_t)\n",
    "out_dog= model(image_null_t)\n",
    "print(\"predicted cat tensor:\", out_cat)\n",
    "print(\"predicted dog tensor:\", out_dog)\n",
    "print(\"\")\n",
    "# Print:\n",
    "if(labels[out_cat.argmax()]== 0):\n",
    "    print(\"smoke\")\n",
    "else:\n",
    "    print(\"else\")\n",
    "    \n",
    "# Show Image:\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "# Print:\n",
    "if(labels[out_dog.argmax()]== 0):\n",
    "    print(\"cat\")\n",
    "else:\n",
    "    print(\"dog\")\n",
    "    \n",
    "# Show Image Null:\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(image_null)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
